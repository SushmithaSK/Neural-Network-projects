{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOTgjo/r97s+gO/Kchck1pJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FAUWBbSFFLK-"
      },
      "outputs": [],
      "source": [
        "#Import libraries and modules\n",
        "#boston_housing dataset is already available in the datasets module under Keras\n",
        "#scikit-learn (sklearn) is a simple and efficient tools for predictive data analysis\n",
        "\n",
        "import pandas\n",
        "from keras.datasets import boston_housing\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the dataset\n",
        "#Note that there are 13 features in X_train per sample\n",
        "# X_train[0] – tuple – vector containing all 13 values\n",
        "# Y_train[0] – 15.2 is the expected output\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
        "\n",
        "# let us view on sample from the features\n",
        "for i in range(5):\n",
        "  print(X_train[i], y_train[i])\n",
        "\n",
        "#output\n",
        "#[  1.23247   0.        8.14      0.        0.538     6.142    91.7\n",
        "#   3.9769    4.      307.       21.      396.9      18.72   ] 15.2\n",
        "\n"
      ],
      "metadata": {
        "id": "xB2zHbbOLGB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train.dtype\n",
        "#dtype('float64')\n",
        "X_train.view"
      ],
      "metadata": {
        "id": "r03LJWdPY_rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test[0], y_test[0])"
      ],
      "metadata": {
        "id": "N9Vh4kGKMoUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define baseline model**\n",
        "\n",
        "Sequential() function will return an empty model. (no layers added)\n",
        "Dense(13 - no of neurons ,Input_dim – 13 dimensional vector as input ,Activation = relu function is used\n",
        "No activation function is used for the output layer because it is a regression problem, and you are interested in predicting numerical values directly without transformation.\n",
        "\n",
        "The efficient ADAM optimization algorithm is used, and a mean squared error loss function is optimized. This will be the same metric you will use to evaluate the performance of the model. It is a desirable metric because taking the square root gives an error value you can directly understand in the context of the problem (thousands of dollars)."
      ],
      "metadata": {
        "id": "zDjD7ZIomRd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def baseline_model():\n",
        " # create model\n",
        " model = Sequential()\n",
        " model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
        " model.add(Dense(1, kernel_initializer='normal'))\n",
        " # Compile model\n",
        " model.compile(loss='mean_squared_error', optimizer='adam')\n",
        " return model\n"
      ],
      "metadata": {
        "id": "iIipwsT6Mqvu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model().summary()\n",
        "\n",
        "'''Model: \"sequential\" - default name given\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #\n",
        "=================================================================\n",
        " dense_2 (Dense)             (None, 13)                182\n",
        "\n",
        " dense_3 (Dense)             (None, 1)                 14\n",
        "\n",
        "=================================================================\n",
        "Total params: 196\n",
        "Trainable params: 196\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________'''"
      ],
      "metadata": {
        "id": "BN0qshemOgMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate model\n",
        "•\tKFold The training set is split into k smaller sets\n",
        "•\tThe following procedure is followed for each of the k “folds”:\n",
        "*   A model is trained using k-1 of the folds as training data\n",
        "*   the resulting model is validated on the remaining part of the data\n",
        "*   Out of 10 sets 9 will be used for training and 1 for validation ,for each of 100 iteration 1 set is left out\n",
        "\n",
        "•\tAs we fine tune the model we need the value to decrease(The values don’t have any meaning as such but we need them to decrease)\n",
        "\n",
        "The Keras wrapper object used in scikit-learn as a regression estimator is called **KerasRegressor**. You create an instance and pass it both the name of the function to create the neural network model and some parameters to pass along to the fit() function of the model later, such as the number of epochs and batch size. Both of these are set to sensible defaults."
      ],
      "metadata": {
        "id": "85i946fxnbPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model\n",
        "\n",
        "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
        "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
        "\n",
        "''' estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
        "Baseline: -24.74 (15.01) MSE'''"
      ],
      "metadata": {
        "id": "FdmxCHdkOZYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running this code gives you an estimate of the model’s performance on the problem for unseen data.\n",
        "\n",
        "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision.\n",
        "\n",
        "The result reports the mean squared error, including the average and standard deviation (average variance) across all ten folds of the cross validation evaluation."
      ],
      "metadata": {
        "id": "9ltDvd9qokxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate model with standardized dataset\n",
        "•\tStandardizing is considered part of pre-processing\n",
        "•\tstandardize is the process of normalizing the different features to a uniform scale.\n",
        "•\tTypically this is done by subtracting the mean and dividing by the standard deviation\n",
        "•\tz = (x - u) / s\n",
        "•\tStandardization of a dataset is a common requirement for many machine learning estimators:\n",
        "they might behave badly if the individual features do not more or less look like standard normally distributed data\n",
        "•\tThough the results might vary, on average, a standardized model usually performs better.\n",
        "•\tKeras automatically standardize. Then train the model(similar steps as before)\n",
        "\n",
        "A further extension of this section would be to similarly apply a rescaling to the output variable, such as normalizing it to the range of 0-1 and using a Sigmoid or similar activation function on the output layer to narrow output predictions to the same range."
      ],
      "metadata": {
        "id": "xsZ2RTCIoo4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model with standardized dataset\n",
        "\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=50, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(pipeline, X_train, y_train, cv=kfold)\n",
        "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
        "\n",
        "#Baseline: -24.74 (15.01) MSE\n",
        "#Standardized: -22.58 (10.71) MSE"
      ],
      "metadata": {
        "id": "lqG1qKY5OZPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tuning with deeper model**\n",
        "\n",
        "•\tTuning is the process of varying the hyperparameters to arrive at a better model\n",
        "•\tOne of the approaches of tuning is by increasing the number of layers\n",
        "•\tWE are adding a layer in between (6 neurons) then rest is same\n",
        "•\tThen how far to go? Till we are satisfied with our error – more layers will increase cost so"
      ],
      "metadata": {
        "id": "_tmumyTneJD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#13 inputs -> [13 -> 6] -> 1 output\n",
        "def larger_model():\n",
        " # create model\n",
        " model = Sequential()\n",
        " model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
        " model.add(Dense(6, kernel_initializer='normal', activation='relu'))\n",
        " model.add(Dense(1, kernel_initializer='normal'))\n",
        " # Compile model\n",
        " model.compile(loss='mean_squared_error', optimizer='adam')\n",
        " return model"
      ],
      "metadata": {
        "id": "AhIk47H0etcX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "larger_model().summary()\n",
        "\n",
        "\"\"\"Model: \"sequential_21\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #\n",
        "=================================================================\n",
        " dense_42 (Dense)            (None, 13)                182\n",
        "\n",
        " dense_43 (Dense)            (None, 6)                 84\n",
        "\n",
        " dense_44 (Dense)            (None, 1)                 7\n",
        "\n",
        "=================================================================\n",
        "Total params: 273 ---increased count as we have added one more layer\n",
        "Trainable params: 273\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\"\"\""
      ],
      "metadata": {
        "id": "mHDfw3tEfMC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=50, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(pipeline, X_train, y_train, cv=kfold)\n",
        "print(\"Larger: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
        "\n",
        "#Baseline: -24.74 (15.01) MSE\n",
        "#Standardized: -22.58 (10.71) MSE\n",
        "#Larger: -17.75 (9.03) MSE"
      ],
      "metadata": {
        "id": "BNijEhJCfiiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tuning with wider model**\n",
        "\n",
        "Tuning is the process of varying the hyperparameters to arrive at a better model\n",
        "One of the approaches of tuning is by increasing the number of neurons in a layer\n",
        "The added lwer has 20 neurons rest is same\n",
        "Error is further reduced.\n"
      ],
      "metadata": {
        "id": "_yfxhXr2hS3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#13 inputs -> [20] -> 1 output\n",
        "def wider_model():\n",
        " # create model\n",
        " model = Sequential()\n",
        " model.add(Dense(20, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
        " model.add(Dense(1, kernel_initializer='normal'))\n",
        " # Compile model\n",
        " model.compile(loss='mean_squared_error', optimizer='adam')\n",
        " return model"
      ],
      "metadata": {
        "id": "gR89SHzMhk3T"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wider_model().summary()\n",
        "\n",
        "\"\"\"Model: \"sequential_33\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #\n",
        "=================================================================\n",
        " dense_78 (Dense)            (None, 20)                280\n",
        "\n",
        " dense_79 (Dense)            (None, 1)                 21\n",
        "\n",
        "=================================================================\n",
        "Total params: 301\n",
        "Trainable params: 301\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\"\"\""
      ],
      "metadata": {
        "id": "UKFqpN13h2gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=50, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(pipeline, X_train, y_train, cv=kfold)\n",
        "print(\"wider: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
        "\n",
        "#Baseline: -24.74 (15.01) MSE\n",
        "#Standardized: -22.58 (10.71) MSE\n",
        "#Larger: -17.75 (9.03) MSE\n",
        "#wider: -16.54 (7.91) MSE\n",
        "\n",
        "#Building the model reveals a further drop in error to about 16 thousand squared dollars. This is not a bad result for this problem."
      ],
      "metadata": {
        "id": "QMcMMf4bh2gY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
